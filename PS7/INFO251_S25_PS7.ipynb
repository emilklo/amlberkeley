{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeku0zphqyuM"
      },
      "source": [
        "To get around Colab's Markdown display issues and potential issues with the `plotly` library, the questions in this problem set are rendered in code blocks. Simply re-run the associated code block to re-render the question, if needed.\n",
        "\n",
        "As a benefit, the questions will come with occasional hints about your programming environment.\n",
        "\n",
        "Make sure the file `PS7.py` is in the same directory as this notebook. This file contains the question definitions and helper functions for this dataset. Do not modify this file.\n",
        "\n",
        "If you do use Google Colab, make the notebook editable for the TAs, and provide the share link to the notebook below. Please share the notebook you worked on, and not a copy of it.\n",
        "\n",
        "**Make sure the 'notify' checkbox is <i>not</i> checked when sharing the notebook.**\n",
        "\n",
        "--------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75jy6BnqyuN"
      },
      "source": [
        "link to Colab: https://colab.research.google.com/github/emilklo/amlberkeley/blob/main/PS7/INFO251_S25_PS7.ipynb#scrollTo=VtHeSO2bqyuP\n",
        "\n",
        "(leave blank if not using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPfSk1tlqyuN"
      },
      "outputs": [],
      "source": [
        "# download necessary packages\n",
        "%pip install sentence-transformers plotly datasets transformers \"transformers[torch]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diTFKcelqyuO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set_style()\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "!wget https://raw.githubusercontent.com/emilklo/amlberkeley/main/PS7/PS7.py\n",
        "!pip install datasets\n",
        "\n",
        "\n",
        "import PS7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcaJi8e5qyuO"
      },
      "outputs": [],
      "source": [
        "PS7.intro()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_l4td6R2f2B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "druJZ3huc09H"
      },
      "outputs": [],
      "source": [
        "PS7.q0()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w335bSWIc09H"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 0 - your answers here</b></span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-LM4CDoqyuO"
      },
      "outputs": [],
      "source": [
        "PS7.part1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e061PHzqyuO"
      },
      "outputs": [],
      "source": [
        "PS7.q1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXKii8_IqyuP"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset='all', remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "# your code here\n",
        "print(f\"Number of documents: {len(newsgroups.data)}\")\n",
        "print(f\"Categories: {newsgroups.target_names}\")\n",
        "print(\"Example document:\")\n",
        "print(newsgroups.data[0])\n",
        "print(f\"Label: {newsgroups.target_names[newsgroups.target[0]]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZi1nqUHqyuP"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 1 - your answers here</b></span>\n",
        "\n",
        "a) The data is a collection of about 19,000 newsgroup posts from 20 online discussion topics. Each data point is a newsgroup post text document. Every document is labeled with one of 20 newsgroup categories.\n",
        "\n",
        "b) For each text post, the prediction target is the category it belongs to.\n",
        "This is represented by the target field in the dataset as an integer which maps to the category in target_names. There are 20 classes to predict.\n",
        "\n",
        "c) No, newsgroups[\"data\"] cannot be passed directly to a LogisticRegression method. newsgroups[\"data\"] is a list of raw text strings, and LogisticRegression expects numerical input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E291F1buc09H"
      },
      "outputs": [],
      "source": [
        "PS7.q2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtHeSO2bqyuP"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "subset_size = 2000\n",
        "\n",
        "# your code here\n",
        "vectorized = fetch_20newsgroups_vectorized(subset='all')\n",
        "X_tfidf = vectorized.data[:subset_size].toarray()\n",
        "print(X_tfidf.shape)\n",
        "\n",
        "raw_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "texts = raw_data.data[:subset_size]\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\tdevice = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "\tdevice = 'mps'\n",
        "else:\n",
        "\tdevice = 'cpu'\n",
        "\n",
        "print(device)\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "truncation_limit = 256\n",
        "num_truncated = 0\n",
        "for text in texts:\n",
        "    num_tokens = len(tokenizer.encode(text, truncation=True, add_special_tokens=True))\n",
        "    if num_tokens >= truncation_limit:\n",
        "        num_truncated += 1\n",
        "print(f\"Estimated number of truncated texts: {num_truncated} out of {subset_size}\")\n",
        "\n",
        "\n",
        "!pip install -q sentence-splitter\n",
        "from sentence_splitter import SentenceSplitter\n",
        "splitter = SentenceSplitter(language='en')\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "document_embeddings = []\n",
        "\n",
        "for doc in tqdm(texts[:subset_size]):\n",
        "    sentences = splitter.split(doc)\n",
        "    if len(sentences) == 0:\n",
        "        document_embeddings.append(np.zeros(384))\n",
        "        continue\n",
        "    sentence_embeddings = model.encode(sentences, batch_size=16, device=device)\n",
        "    doc_embedding = np.mean(sentence_embeddings, axis=0)\n",
        "    document_embeddings.append(doc_embedding)\n",
        "\n",
        "X_embed = np.vstack(document_embeddings)\n",
        "print(\"X_embed shape:\", X_embed.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A20el0quqyuP"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 2 - your answers here</b></span>\n",
        "\n",
        "a) 484 data points out of the first 2000 run into the models truncation limit\n",
        "\n",
        "b) (2000, 130107) and (2000, 384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SZmFCf1c09I"
      },
      "outputs": [],
      "source": [
        "PS7.q3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCrJUGsdqyuP"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# your code here\n",
        "pca_tfidf = PCA(n_components=100, svd_solver='randomized', random_state=42)\n",
        "var_tfidf = pca_tfidf.fit(X_tfidf).explained_variance_ratio_\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, 101), var_tfidf)\n",
        "plt.title('TF-IDF')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n",
        "\n",
        "pca_embed = PCA(n_components=100, svd_solver='randomized', random_state=42)\n",
        "var_embed = pca_embed.fit(X_embed).explained_variance_ratio_\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, 101), var_embed)\n",
        "plt.title('Embeddings')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n",
        "\n",
        "# ——— Variation lost if only first 3 PCs are kept ———\n",
        "loss_tfidf = 1 - var_tfidf[:3].sum()\n",
        "loss_embed = 1 - var_embed[:3].sum()\n",
        "\n",
        "print(f\"TF-IDF: Variation lost with 3 PCs: {loss_tfidf:.2%}\")\n",
        "print(f\"Embeddings:Variation lost with 3 PCs: {loss_embed:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCP03B47qyuP"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 3 - your answers here</b></span>\n",
        "\n",
        "a) For TF-IDF the curve starts above 7% for the first PC, then drops to around 2%, and continues a very gradual decreasing tail towards near 0 by PC 100. This tail shows that the tf-idf space is very high-dimensional and variance is spread over thousands of features, so each component beyond the first few captures only a tiny sliver of variance.\n",
        "\n",
        "For the embeddings variant the first PC captures roughly 10%, the second about 3.2 %, the third 2.0 %, and then it falls off more steeply before settling into a lower tail. This indicates the 384-dimensional embedding space is much more compressible: a handful of PCs explain a sizeable chunk of the variance, unlike the tf-idf case.\n",
        "\n",
        "b) TF-IDF\n",
        "You lose about 88.3 % of the total variance, retaining only ∼11.7 % in the first three components.\n",
        "\n",
        "Embeddings\n",
        "You lose about 79.8 % of the variance, retaining ∼20.2 % in the first three components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oniu748Tc09I"
      },
      "outputs": [],
      "source": [
        "PS7.q4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_QRP_GHqyuQ"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# your code here\n",
        "labels = raw_data.target[:subset_size]\n",
        "names = raw_data.target_names\n",
        "\n",
        "# ——— PCA to 3 dims ———\n",
        "pca3_tfidf = PCA(n_components=3, random_state=42)\n",
        "X_tfidf_3 = pca3_tfidf.fit_transform(X_tfidf)\n",
        "\n",
        "pca3_embed = PCA(n_components=3, random_state=42)\n",
        "X_embed_3 = pca3_embed.fit_transform(X_embed)\n",
        "\n",
        "# ——— Verify explained‐variance sums ———\n",
        "sum3_from100_tfidf = var_tfidf[:3].sum()                  # from your 100‐PC run\n",
        "sum3_from3_tfidf  = pca3_tfidf.explained_variance_ratio_.sum()\n",
        "print(f\"TF-IDF: 100‐PC sum3 = {sum3_from100_tfidf:.4f}, 3‐PC fit sum = {sum3_from3_tfidf:.4f}\")\n",
        "\n",
        "sum3_from100_embed = var_embed[:3].sum()\n",
        "sum3_from3_embed  = pca3_embed.explained_variance_ratio_.sum()\n",
        "print(f\"Embed: 100‐PC sum3 = {sum3_from100_embed:.4f}, 3‐PC fit sum = {sum3_from3_embed:.4f}\")\n",
        "\n",
        "# ——— 3D Scatter for TF-IDF ———\n",
        "fig_tfidf = px.scatter_3d(\n",
        "    x=X_tfidf_3[:,0], y=X_tfidf_3[:,1], z=X_tfidf_3[:,2],\n",
        "    color=[names[i] for i in labels],\n",
        "    title=\"3D PCA of TF-IDF Features\",\n",
        "    labels={'x':'PC1','y':'PC2','z':'PC3'}\n",
        ")\n",
        "fig_tfidf.show()\n",
        "\n",
        "# ——— 3D Scatter for Embeddings ———\n",
        "fig_embed = px.scatter_3d(\n",
        "    x=X_embed_3[:,0], y=X_embed_3[:,1], z=X_embed_3[:,2],\n",
        "    color=[names[i] for i in labels],\n",
        "    title=\"3D PCA of Sentence Embeddings\",\n",
        "    labels={'x':'PC1','y':'PC2','z':'PC3'}\n",
        ")\n",
        "fig_embed.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlTTTKHPqyuQ"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 4 - your answers here</b></span>\n",
        "\n",
        "a) Yes. When I sum the first three explained‐variance ratios from the 100-component PCA and compare it to the variance from a direct 3-component PCA, I get the exact same numbers. This shows that PCA always allocates the same total variance to the top k components whether I ask for k directly or slice them off a larger decomposition.\n",
        "\n",
        "b) Yes. The first three eigenvectors of the covariance matrix are the same for 3 or 100.\n",
        "\n",
        "c) For the tf-idf plot, the data points almost looks like they are scattered randomly. Light green for example, can be seen top left and right, bottom left and right, and center. Clusters bleed into each other because raw word count patterns capture surface overlap. “drive” appears in both comp.sys.ibm.pc.hardware and rec.motorcycles.\n",
        "\n",
        "For the embeddings plot, the clusters are noticeably tighter and more separated, reflecting that the MiniLM model has learned deeper semantic distinctions. It looks like some groups have clustered together, indicating that some topics have similarities with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIgrg2gnc09I"
      },
      "outputs": [],
      "source": [
        "PS7.part2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbu0PT8ac09I"
      },
      "outputs": [],
      "source": [
        "PS7.q5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYirFbQyqyuQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "\n",
        "def k_means_clustering(X, k, random_state=None):\n",
        "    # your code here\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    # choosing centroids\n",
        "    init_idx = rng.choice(X.shape[0], size=k, replace=False)\n",
        "    centroids = X[init_idx]\n",
        "\n",
        "    for i in range(1, 101):\n",
        "        labels = pairwise_distances_argmin(X, centroids)\n",
        "\n",
        "        new_centroids = np.vstack([\n",
        "            X[labels == j].mean(axis=0) if np.any(labels == j) else centroids[j]\n",
        "            for j in range(k)\n",
        "        ])\n",
        "\n",
        "        # convergance check\n",
        "        shift = np.linalg.norm(new_centroids - centroids, axis=1).max()\n",
        "        centroids = new_centroids\n",
        "        if shift < 1e-4:\n",
        "            break\n",
        "\n",
        "    return centroids, labels, i\n",
        "\n",
        "k = 20\n",
        "centroids, labels, n_iters = k_means_clustering(X_embed_3, k)\n",
        "\n",
        "print(f\"Converged in {n_iters} iterations\")\n",
        "print(f\"Centroids shape: {centroids.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AuPSlkoc09I"
      },
      "outputs": [],
      "source": [
        "PS7.q6()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ln1aUMNqyuQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "# visualize the synthetic dataset to choose the number of clusters\n",
        "# plt.scatter(X[:, 0], X[:, 1], s=50)\n",
        "\n",
        "# your code here\n",
        "# Run custom k-means\n",
        "k = 4\n",
        "centroids, labels, n_iters = k_means_clustering(X, k)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='tab10', edgecolor='k', alpha=0.7)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "plt.title(f'K-Means Clustering on Synthetic Data (k={k})')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnVkxfdec09I"
      },
      "outputs": [],
      "source": [
        "PS7.q7()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSUWKM-LqyuQ"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "def davies_bouldin_index(X, labels, centroids):\n",
        "    k = centroids.shape[0]\n",
        "    # 1) Compute scatter S_i for each cluster = average distance to its centroid\n",
        "    S = np.zeros(k)\n",
        "    for i in range(k):\n",
        "        members = X[labels == i]\n",
        "        if len(members) > 0:\n",
        "            dists = np.linalg.norm(members - centroids[i], axis=1)\n",
        "            S[i] = dists.mean()\n",
        "        else:\n",
        "            S[i] = 0.0\n",
        "\n",
        "    # 2) Compute R_ij = (S_i + S_j) / d(c_i, c_j)\n",
        "    #    then R_i = max_{j != i} R_ij\n",
        "    R = np.zeros((k, k))\n",
        "    D = np.linalg.norm(centroids[:, None] - centroids[None, :], axis=2)  # pairwise centroid–centroid\n",
        "    for i in range(k):\n",
        "        for j in range(k):\n",
        "            if i != j and D[i, j] > 0:\n",
        "                R[i, j] = (S[i] + S[j]) / D[i, j]\n",
        "    R_i = R.max(axis=1)\n",
        "\n",
        "    # 3) DB index = average of R_i\n",
        "    return R_i.mean()\n",
        "\n",
        "# ——— Sweep k from 2 to 15 ———\n",
        "db_scores = []\n",
        "ks = list(range(2, 16))\n",
        "for k in ks:\n",
        "    centroids, labels, _ = k_means_clustering(X_embed_3, k)    # your k_means on 3-D data\n",
        "    db = davies_bouldin_index(X_embed_3, labels, centroids)\n",
        "    db_scores.append(db)\n",
        "\n",
        "# ——— Plot DB index vs k ———\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(ks, db_scores, marker='o')\n",
        "plt.xticks(ks)\n",
        "plt.xlabel(\"Number of clusters k\")\n",
        "plt.ylabel(\"Davies–Bouldin index (lower is better)\")\n",
        "plt.title(\"DB Index vs k on 3-D PCA Embeddings\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7flWZtlqyuQ"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 7 - your answers here</b></span>\n",
        "\n",
        "c) According to the index, the sweet spot is 8 clusters, which is fewer than the 20 original newsgroup labels. This tells us that while there are twenty topics, the data really condenses into a handful of broader themes. It aligns perfectly with what q4c told us: documents naturally group into larger topics like computer forums, science, and recreation rather than twenty completely separate islands. The eight clusters capture the major semantic divides observed in the 3D PCA plot for embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jsX82bZc09J"
      },
      "outputs": [],
      "source": [
        "PS7.part3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaxpytwhc09J"
      },
      "outputs": [],
      "source": [
        "PS7.q8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XEV-gh5uA-0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from PS7 import newsgroups_tokenized_datasets\n",
        "\n",
        "# tokenize the training and validation datasets and handle padding tokens (done for you)\n",
        "base_model = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "trn_dataset, val_dataset = newsgroups_tokenized_datasets(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWArbUGvqyuQ"
      },
      "outputs": [],
      "source": [
        "# instantiate the model\n",
        "\n",
        "# hint: pass in pad_token_id=tokenizer.pad_token_id\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model,\n",
        "    num_labels=20,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# hyperparameters\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./ft-distilgpt2\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=trn_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(f\"Validation accuracy: {metrics['eval_accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_metrics = trainer.evaluate(eval_dataset=trn_dataset)\n",
        "print(f\"Train accuracy: {train_metrics['eval_accuracy']:.2%}\")"
      ],
      "metadata": {
        "id": "MQXXR1BvvKJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H07ayZ5qyuR"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 8 - your answers here</b></span>\n",
        "\n",
        "hyperparameters:\n",
        "\n",
        "Epochs: 3\n",
        "\n",
        "Batch sizes: 16\n",
        "\n",
        "Learning rate: 2 * 10^-5\n",
        "\n",
        "Weight decay: 0.01\n",
        "\n",
        "train accuracy: 76.37%\n",
        "\n",
        "test accuracy: 67.26%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsOcxK65c09J"
      },
      "outputs": [],
      "source": [
        "PS7.q9()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gh-3zZLc09J"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "tfidf_train = fetch_20newsgroups_vectorized(subset='train')\n",
        "X_tfidf_train = tfidf_train.data.toarray()\n",
        "y_train = tfidf_train.target\n",
        "\n",
        "tfidf_test = fetch_20newsgroups_vectorized(subset='test')\n",
        "X_tfidf_test = tfidf_test.data.toarray()\n",
        "y_test = tfidf_test.target\n",
        "\n",
        "lr_tfidf = LogisticRegressionCV(\n",
        "    Cs=[0.01, 0.1, 1, 10],\n",
        "    cv=5,\n",
        "    penalty='l2',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "lr_tfidf.fit(X_tfidf_train, y_train)\n",
        "\n",
        "train_acc_tfidf = lr_tfidf.score(X_tfidf_train, y_train)\n",
        "test_acc_tfidf = lr_tfidf.score(X_tfidf_test,  y_test)\n",
        "best_C_tfidf = lr_tfidf.C_[0]\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "\n",
        "raw_train = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "X_embed_train = model.encode(raw_train.data, batch_size=32, show_progress_bar=True)\n",
        "y_train_embed = raw_train.target\n",
        "\n",
        "raw_test = fetch_20newsgroups(subset='test', remove=('headers','footers','quotes'))\n",
        "X_embed_test = model.encode(raw_test.data,  batch_size=32, show_progress_bar=True)\n",
        "y_test_embed = raw_test.target\n",
        "\n",
        "lr_embed = LogisticRegressionCV(\n",
        "    Cs=[0.01, 0.1, 1, 10],\n",
        "    cv=5,\n",
        "    penalty='l2',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "lr_embed.fit(X_embed_train, y_train_embed)\n",
        "\n",
        "train_acc_embed = lr_embed.score(X_embed_train, y_train_embed)\n",
        "test_acc_embed = lr_embed.score(X_embed_test,  y_test_embed)\n",
        "best_C_embed = lr_embed.C_[0]\n",
        "\n",
        "# ——— 5) Report results ———\n",
        "print(\"TF-IDF Logistic Regression\")\n",
        "print(f\" • Best C      = {best_C_tfidf}\")\n",
        "print(f\" • Train acc   = {train_acc_tfidf:.2%}\")\n",
        "print(f\" • Test  acc   = {test_acc_tfidf:.2%}\\n\")\n",
        "\n",
        "print(\"MiniLM Embeddings Logistic Regression\")\n",
        "print(f\" • Best C      = {best_C_embed}\")\n",
        "print(f\" • Train acc   = {train_acc_embed:.2%}\")\n",
        "print(f\" • Test  acc   = {test_acc_embed:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTA7ZJuec09J"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Question 9 - your answers here</b></span>\n",
        "\n",
        "tf-idf hyperparameters:\n",
        "\n",
        "tf-idf train accuracy:\n",
        "\n",
        "tf-idf test accuracy:\n",
        "\n",
        "miniLM hyperparameters:\n",
        "\n",
        "miniLM train accuracy:\n",
        "\n",
        "miniLM test accuracy:\n",
        "\n",
        "discussion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjP9VSRc09J"
      },
      "outputs": [],
      "source": [
        "PS7.ec1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5eH2Gz4qyuR"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRu6iEbjzIrG"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Extra credit 1 - your answers here</b></span>\n",
        "\n",
        "a)\n",
        "\n",
        "b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo83LC01c09P"
      },
      "outputs": [],
      "source": [
        "PS7.ec2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCI5XP8Ec09P"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_embedding):\n",
        "        \"\"\"\n",
        "        d_embedding: int, the dimension of the input embedding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # your code here\n",
        "        ...\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # your code here\n",
        "        ...\n",
        "\n",
        "def attention_classifier(num_classes):\n",
        "    # create an embedding layer and freeze it so that it won't be updated\n",
        "    embed = nn.Embedding.from_pretrained(..., freeze=True)\n",
        "    d_embedding = embed.embedding_dim\n",
        "\n",
        "    # your implemented attention layer\n",
        "    attn = SelfAttention(d_embedding=d_embedding)\n",
        "\n",
        "    # classifier head\n",
        "    return nn.Sequential(\n",
        "        embed,                      # tokens -> word vectors\n",
        "        attn,                       # self‑attention\n",
        "        nn.AdaptiveAvgPool1d(1),    # mean-pool over the sequence of words\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(d_embedding, num_classes)  # classifier\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPm9ID3Zc09P"
      },
      "outputs": [],
      "source": [
        "PS7.ec3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoXfPLR10GaT"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N25-kuZ90IG6"
      },
      "source": [
        "<span style=\"color:#FDB515\"><b>Extra credit 3 - your answers here</b></span>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}